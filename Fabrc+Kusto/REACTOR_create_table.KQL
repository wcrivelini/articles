
// Create the table with the TRAFFIC information.
// The data loading process estimated to take ~3-4min to complete (114M+ rows of data).
// Notes: VIN - is Vehicle ID 
.create-merge table Traffic (Timestamp:datetime, VIN:string, Ave:int, Street:int)

//clear any previously ingested data if such exists
.clear table Traffic data
.ingest async into table Traffic (@'https://kustodetectiveagency.blob.core.windows.net/digitown-traffic/log_00000.csv.gz')
.ingest async into table Traffic (@'https://kustodetectiveagency.blob.core.windows.net/digitown-traffic/log_00001.csv.gz')
.ingest into table Traffic (@'https://kustodetectiveagency.blob.core.windows.net/digitown-traffic/log_00002.csv.gz')





// create  table LOGSRAW
.create table logsRaw(Timestamp:datetime, Source:string, Node:string, Level:string, Component:string, ClientRequestId:string, Message:string, Properties:dynamic) 


//run KQL
logsRaw
| count


//run SQL
//select count() from logsRaw;

//ingest data from mutiple files
.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/00/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv', creationTime='2014-03-08T00:00:00Z');
// copy the Operation ID that identifies previous instruction and run ".show operations [ID]" to follow up the status of that process
.show operations bc5e5d34-8644-4556-860c-bbbdffe6a358

.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/01/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv', creationTime='2014-03-08T01:00:00Z');

.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/02/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv', creationTime='2014-03-08T02:00:00Z');

.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/03/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv', creationTime='2014-03-08T03:00:00Z');

.ingest async into table logsRaw (h'https://logsbenchmark00.blob.core.windows.net/logsbenchmark-onegb/2014/03/08/04/data.csv.gz?sp=rl&st=2022-08-18T00:00:00Z&se=2030-01-01T00:00:00Z&spr=https&sv=2021-06-08&sr=c&sig=5pjOow5An3%2BTs5mZ%2FyosJBPtDvV7%2FXfDO8pLEeeylVc%3D') with (format='csv', creationTime='2014-03-08T04:00:00Z');



//run KQL
logsRaw
| count


// piechart (level vs count)
logsRaw
| summarize events = count() by Level
| render piechart 


// timeline - using bins
logsRaw
| where Timestamp between ( datetime(2014-03-08 00:00) .. datetime(2014-03-09 00:00) )
| extend point = bin(Timestamp, 30min)
| summarize cnt = count() by point
| render timechart 


.show journal 
| where ChangeCommand has "ManiputatelogsRaw"

//reading JSON (contents from array "Properties") and plotting timeline
let TimeBuckets = 30min;
logsRaw 
| extend Size = tolong(Properties.size)
| make-series MySeries=round(avg(Size),2) on Timestamp step TimeBuckets by Level
| render timechart



// looking for OUTLIERS (Anomalies) for the same data
let TimeBuckets = 1m;
logsRaw
| extend Size = tolong(Properties.size)
| make-series ActualSize=round(avg(Size),2) on Timestamp step TimeBuckets
| extend anomaly = series_decompose_anomalies(ActualSize)
| render anomalychart with(anomalycolumns=anomaly, title='Ingestion Anomalies')
